{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Data Integration with Indicator Kriging \n",
    "\n",
    "#### Michael Pyrcz, Professor, The University of Texas at Austin \n",
    "\n",
    "\n",
    "#### Contacts: [Twitter/@GeostatsGuy](https://twitter.com/geostatsguy) | [GitHub/GeostatsGuy](https://github.com/GeostatsGuy) | [www.michaelpyrcz.com](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)\n",
    "\n",
    "#### Load the required libraries\n",
    "\n",
    "The following code loads the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "supress_warnings = False\n",
    "\n",
    "import os                                                 # to set current working directory \n",
    "import numpy as np                                        # arrays and matrix math\n",
    "import pandas as pd                                       # DataFrames\n",
    "import matplotlib.pyplot as plt                           # plotting\n",
    "from scipy.interpolate import make_interp_spline          # smooth curves\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks\n",
    "\n",
    "import geostatspy.geostats as geostats\n",
    "import geostatspy.GSLIB as GSLIB\n",
    "\n",
    "plt.rc('axes', axisbelow=True)                            # grid behind plotting elements\n",
    "if supress_warnings == True:\n",
    "    import warnings                                       # supress any warnings for this demonstration\n",
    "    warnings.filterwarnings('ignore')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the working directory\n",
    "\n",
    "I always like to do this so I don't lose files and to simplify subsequent read and writes (avoid including the full address each time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\17137\\\\OneDrive - The University of Texas at Austin\\\\Courses\\\\DataSets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m17137\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive - The University of Texas at Austin\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCourses\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDataSets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\17137\\\\OneDrive - The University of Texas at Austin\\\\Courses\\\\DataSets'"
     ]
    }
   ],
   "source": [
    "os.chdir(r\"C:\\Users\\17137\\OneDrive - The University of Texas at Austin\\Courses\\DataSets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data\n",
    "\n",
    "Here's the command to load our comma delimited data file in to a Pandas' DataFrame object.  For fun try misspelling the name. You will get an ugly, long error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/SoftData_Contact2D.csv')     # load our data table \n",
    "df = pd.read_csv('SoftData_Contact2D.csv')     # load our data table (wrong name!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loaded our file into our DataFrame called 'df'. But how do you really know that it worked? Visualizing the DataFrame would be useful and we already leard about these methods in this demo (https://git.io/fNgRW). \n",
    "\n",
    "We can preview the DataFrame by printing a slice or by utilizing the 'head' DataFrame member function (with a nice and clean format, see below). With the slice we could look at any subset of the data table and with the head command, add parameter 'n=13' to see the first 13 rows of the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(n=13)                           # we could also use this command for a table preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xmin = 0.0; xmax = 1000.0               # range of x values\n",
    "# ymin = 0.0; ymax = 1000.0               # range of y values\n",
    "\n",
    "# xsiz = 10; ysiz = 10\n",
    "# nx = 100; ny = 100\n",
    "# xmn = 5; ymn = 5\n",
    "\n",
    "# cmap = plt.cm.plasma                    # color map\n",
    "# GSLIB.locmap(df,'X','Y','Facies',xmin,xmax,ymin,ymax,0,1,'Well Data - Facies','X(m)','Y(m)','Facies (0 - shale, 1 - sand)',cmap,'locmap_facies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get a package import error, you may have to first install some of these packages. This can usually be accomplished by opening up a command window on Windows and then typing 'python -m pip install [package-name]'. More assistance is available with the respective package docs.  \n",
    "\n",
    "#### Declare functions\n",
    "\n",
    "These are some of the functions from GeostatsPy required by the new program.  We will declare them here and then in the future integrate the new indicator kriging program into the package properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_rotmat(c0, nst, it, cc, ang, pmx):\n",
    "\n",
    "    PI = 3.141_592_65\n",
    "    DTOR = PI / 180.0\n",
    "\n",
    "    # The first time around, re-initialize the cosine matrix for the variogram\n",
    "    # structures\n",
    "    rotmat = np.zeros((4, nst))\n",
    "    maxcov = c0\n",
    "    for js in range(0, nst):\n",
    "        azmuth = (90.0 - ang[js]) * DTOR\n",
    "        rotmat[0, js] = math.cos(azmuth)\n",
    "        rotmat[1, js] = math.sin(azmuth)\n",
    "        rotmat[2, js] = -1 * math.sin(azmuth)\n",
    "        rotmat[3, js] = math.cos(azmuth)\n",
    "        if it[js] == 4:\n",
    "            maxcov = maxcov + pmx\n",
    "        else:\n",
    "            maxcov = maxcov + cc[js]\n",
    "    return rotmat, maxcov\n",
    "\n",
    "def ksol_numpy(neq, a, r):\n",
    "    a = a[0: neq * neq]  # trim the array\n",
    "    a = np.reshape(a, (neq, neq))  # reshape to 2D\n",
    "    ainv = linalg.inv(a)  # invert matrix\n",
    "    r = r[0: neq]  # trim the array\n",
    "    s = np.matmul(ainv, r)  # matrix multiplication\n",
    "    return s\n",
    "\n",
    "def cova2(x1, y1, x2, y2, nst, c0, pmx, cc, aa, it, ang, anis, rotmat, maxcov):\n",
    "    EPSLON = 0.000_000\n",
    "    # Check for very small distance\n",
    "    dx = x2 - x1\n",
    "    dy = y2 - y1\n",
    "    if (dx * dx + dy * dy) < EPSLON:\n",
    "        cova2_ = maxcov\n",
    "        return cova2_\n",
    "\n",
    "    # Non-zero distance, loop over all the structures\n",
    "    cova2_ = 0.0\n",
    "    for js in range(0, nst):\n",
    "        # Compute the appropriate structural distance\n",
    "        dx1 = dx * rotmat[0, js] + dy * rotmat[1, js]\n",
    "        dy1 = (dx * rotmat[2, js] + dy * rotmat[3, js]) / anis[js]\n",
    "        h = math.sqrt(max((dx1 * dx1 + dy1 * dy1), 0.0))\n",
    "#        print('cova h'); print(h)\n",
    "        if it[js] == 1:\n",
    "            # Spherical model\n",
    "            hr = h / aa[js]\n",
    "            if hr < 1.0:\n",
    "                cova2_ = cova2_ + cc[js] * (1.0 - hr * (1.5 - 0.5 * hr * hr))\n",
    "        elif it[js] == 2:\n",
    "            # Exponential model\n",
    "            cova2_ = cova2_ + cc[js] * np.exp(-3.0 * h / aa[js])\n",
    "        elif it[js] == 3:\n",
    "            # Gaussian model\n",
    "            hh = -3.0 * (h * h) / (aa[js] * aa[js])\n",
    "            cova2_ = cova2_ + cc[js] * np.exp(hh)\n",
    "        elif it[js] == 4:\n",
    "            # Power model\n",
    "            cov1 = pmx - cc[js] * (h ** aa[js])\n",
    "            cova2_ = cova2_ + cov1\n",
    "    return cova2_\n",
    "\n",
    "def add_grid():\n",
    "    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n",
    "    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n",
    "    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the new indicator kriging program along with a couple of supporting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial as sp         # for fast nearest nearbour search\n",
    "import math  # for trig functions etc.\n",
    "import numpy.linalg as linalg  # for linear algebra\n",
    "\n",
    "def correct_trend(trend):\n",
    "    ny = trend.shape[0]\n",
    "    nx = trend.shape[1]\n",
    "    ncut = trend.shape[2]\n",
    "    for iy in range(0,ny):\n",
    "        for ix in range(0,nx):\n",
    "            sum = 0.0\n",
    "            for ic in range(0,ncut):\n",
    "                sum = sum + trend[iy,ix,ic]\n",
    "            if sum > 0.0:\n",
    "                for icut in range(0,ncut):\n",
    "                    trend[iy,ix,ic] = trend[iy,ix,ic] / sum\n",
    "    return trend\n",
    "\n",
    "def ordrel(ivtype,ncut,ccdf):\n",
    "#    print('input ordering relations'); print(ccdf)\n",
    "    ccdfo = np.zeros(ncut)\n",
    "    print('Inside ordel' + str(ccdf))\n",
    "    ccdf1 = np.zeros(ncut)\n",
    "    ccdf2 = np.zeros(ncut) # do we need MAXCUT = 100 for these 2?\n",
    "\n",
    "# Make sure conditional cdf is within [0,1]:\n",
    "    for i in range(0,ncut):\n",
    "        if ccdf[i] < 0.0:\n",
    "            ccdf1[i] = 0.0\n",
    "            ccdf2[i] = 0.0\n",
    "        elif ccdf[i] > 1.0:\n",
    "            ccdf1[i] = 1.0\n",
    "            ccdf2[i] = 1.0\n",
    "        else:\n",
    "            ccdf1[i] = ccdf[i]\n",
    "            ccdf2[i] = ccdf[i]\n",
    "#    print('ordering relations'); print(ccdf1,ccdf2)\n",
    "\n",
    "# Correct sequentially up, then down, and then average:\n",
    "    if ivtype == 0:\n",
    "        sumcdf = 0.0\n",
    "        for i in range(0,ncut):\n",
    "            sumcdf = sumcdf + ccdf1[i]\n",
    "        if sumcdf <= 0.0: sumcdf = 1.0\n",
    "        for i in range(0,ncut):\n",
    "            ccdfo[i] = ccdf1[i] / sumcdf\n",
    "    else:\n",
    "        for i in range(1,ncut):\n",
    "            if ccdf1[i] < ccdf1[i-1]: ccdf1[i] = ccdf1[i-1]\n",
    "        for i in range(ncut-2,0,-1):\n",
    "            if ccdf2[i] > ccdf2[i+1]: ccdf2[i] = ccdf2[i+1]\n",
    "        for i in range(0,ncut):\n",
    "            ccdfo[i] = 0.5*(ccdf1[i]+ccdf2[i])\n",
    "\n",
    "    print('ccdf1' + str(ccdf1))\n",
    "    print('ccdf2' + str(ccdf2))\n",
    "    print('ccdfo' + str(ccdfo))\n",
    "            \n",
    "# Return with corrected CDF:\n",
    "    return ccdfo\n",
    "\n",
    "def ik2d(df,xcol,ycol,vcol,readind,colstart,ivtype,koption,ncut,thresh,gcdf,trend,tmin,tmax,nx,xmn,xsiz,ny,ymn,ysiz,\n",
    "         ndmin,ndmax,radius,ktype,vario): \n",
    "\n",
    "# Find the needed paramters:\n",
    "    PMX = 9999.9\n",
    "    MAXSAM = ndmax + 1\n",
    "    MAXEQ = MAXSAM + 1\n",
    "    mik = 0  # full indicator kriging\n",
    "    use_trend = False\n",
    "    if trend.shape[0] == nx and trend.shape[1] == ny and trend.shape[2] == ncut: use_trend = True\n",
    "    \n",
    "# load the variogram\n",
    "    MAXNST = 2\n",
    "    nst = np.zeros(ncut,dtype=int); c0 = np.zeros(ncut); cc = np.zeros((MAXNST,ncut)) \n",
    "    aa = np.zeros((MAXNST,ncut),dtype=int); it = np.zeros((MAXNST,ncut),dtype=int) \n",
    "    ang = np.zeros((MAXNST,ncut)); anis = np.zeros((MAXNST,ncut))\n",
    "\n",
    "    for icut in range(0,ncut):\n",
    "        nst[icut] = int(vario[icut]['nst'])\n",
    "        c0[icut] = vario[icut]['nug']; cc[0,icut] = vario[icut]['cc1']; it[0,icut] = vario[icut]['it1']; \n",
    "        ang[0,icut] = vario[icut]['azi1']; \n",
    "        aa[0,icut] = vario[icut]['hmaj1']; anis[0,icut] = vario[icut]['hmin1']/vario[icut]['hmaj1'];\n",
    "        if nst[icut] == 2:\n",
    "            cc[1,icut] = vario[icut]['cc2']; it[1,icut] = vario[icut]['it2']; ang[1,icut] = vario[icut]['azi2']; \n",
    "            aa[1,icut] = vario[icut]['hmaj2']; anis[1,icut] = vario[icut]['hmin2']/vario[icut]['hmaj2'];\n",
    "\n",
    "# Load the data\n",
    "    if readind is True:\n",
    "        df_extract = df\n",
    "    else:\n",
    "        df_extract = df.loc[(df[vcol] >= tmin) & (df[vcol] <= tmax)]    # trim values outside tmin and tmax\n",
    "    MAXDAT = len(df_extract)\n",
    "    MAXCUT = ncut\n",
    "    MAXNST = 2\n",
    "    MAXROT = MAXNST*MAXCUT+ 1\n",
    "    ikout = np.zeros((ny,nx,ncut))\n",
    "    maxcov = np.zeros(ncut)\n",
    "            \n",
    "    # Allocate the needed memory:   \n",
    "    xa = np.zeros(MAXSAM)\n",
    "    ya = np.zeros(MAXSAM)\n",
    "    vra = np.zeros(MAXSAM)\n",
    "    dist = np.zeros(MAXSAM)\n",
    "    nums = np.zeros(MAXSAM)\n",
    "    r = np.zeros(MAXEQ)\n",
    "    rr = np.zeros(MAXEQ)\n",
    "    s = np.zeros(MAXEQ)\n",
    "    a = np.zeros(MAXEQ*MAXEQ)\n",
    "    print('Grid set up' + str(ny) + ' ' + str(nx))\n",
    "    vr = np.zeros((MAXDAT,MAXCUT+1))\n",
    "    print(\"MAXCUT+1 \" + str(MAXCUT+1))\n",
    "    \n",
    "    nviol = np.zeros(MAXCUT)\n",
    "    aviol = np.zeros(MAXCUT)\n",
    "    xviol = np.zeros(MAXCUT)\n",
    "    \n",
    "    ccdf = np.zeros(ncut)\n",
    "    ccdfo = np.zeros(ncut)\n",
    "    ikout = np.zeros((ny,nx,ncut))\n",
    "    \n",
    "    x = df_extract[xcol].values\n",
    "    y = df_extract[ycol].values\n",
    "    v = df_extract[vcol].values\n",
    "    print('Data values ' + str(v))\n",
    "    \n",
    "# The indicator data are constructed knowing the thresholds and the\n",
    "# data value.\n",
    "\n",
    "    if readind == True:\n",
    "        for idata in range(0,len(df)):\n",
    "            for icut in range(0,ncut):\n",
    "                print(icut+colstart)\n",
    "                vr[idata,icut] = df.iloc[idata,icut+colstart]\n",
    "                \n",
    "        print('Read in soft data:')\n",
    "\n",
    "    else:\n",
    "        if ivtype == 0:\n",
    "            for icut in range(0,ncut): \n",
    "                vr[:,icut] = np.where((v <= thresh[icut] + 0.5) & (v > thresh[icut] - 0.5), '1', '0')\n",
    "        else:\n",
    "            for icut in range(0,ncut): \n",
    "                vr[:,icut] = np.where(v <= thresh[icut], '1', '0')\n",
    "    \n",
    "    vr[:,ncut] = v\n",
    "    print(vr)\n",
    "    \n",
    "# Make a KDTree for fast search of nearest neighbours   \n",
    "    dp = list((y[i], x[i]) for i in range(0,MAXDAT))\n",
    "    data_locs = np.column_stack((y,x))\n",
    "    tree = sp.cKDTree(data_locs, leafsize=16, compact_nodes=True, copy_data=False, balanced_tree=True)\n",
    "    \n",
    "# Summary statistics of the input data\n",
    "    \n",
    "    avg = vr[:,ncut].mean()\n",
    "    stdev = vr[:,ncut].std()\n",
    "    ss = stdev**2.0\n",
    "    vrmin = vr[:,ncut].min()\n",
    "    vrmax = vr[:,ncut].max()\n",
    "    print('Data for IK3D: Variable column ' + str(vcol))\n",
    "    print('  Number   = ' + str(MAXDAT))\n",
    "    ndh = MAXDAT\n",
    "    \n",
    "    actloc = np.zeros(MAXDAT, dtype = int)\n",
    "    for i in range(1,MAXDAT):\n",
    "        actloc[i] = i\n",
    "    \n",
    "# Set up the rotation/anisotropy matrices that are needed for the\n",
    "# variogram and search:\n",
    "\n",
    "    print('Setting up rotation matrices for variogram and search')\n",
    "    radsqd = radius * radius\n",
    "    rotmat = []\n",
    "    for ic in range(0,ncut):  \n",
    "        rotmat_temp, maxcov[ic] = setup_rotmat(c0[ic],int(nst[ic]),it[:,ic],cc[:,ic],ang[:,ic],9999.9)\n",
    "        rotmat.append(rotmat_temp)    \n",
    "# Initialize accumulators:  # not setup yet\n",
    "    nk = 0\n",
    "    xk = 0.0\n",
    "    vk = 0.0\n",
    "    for icut in range (0,ncut):\n",
    "        nviol[icut] =  0\n",
    "        aviol[icut] =  0.0\n",
    "        xviol[icut] = -1.0\n",
    "    nxy   = nx*ny\n",
    "    print('Working on the kriging')\n",
    "\n",
    "# Report on progress from time to time:\n",
    "    if koption == 0: \n",
    "        nxy   = nx*ny\n",
    "        nloop = nxy\n",
    "        irepo = max(1,min((nxy/10),10000))\n",
    "    else:\n",
    "        nloop = 10000000\n",
    "        irepo = max(1,min((nd/10),10000))\n",
    "    ddh = 0.0\n",
    "    \n",
    "# MAIN LOOP OVER ALL THE BLOCKS IN THE GRID:\n",
    "    for index in range(0,nloop):\n",
    "      \n",
    "        if (int(index/irepo)*irepo) == index: print('   currently on estimate ' + str(index))\n",
    "    \n",
    "        if koption == 0:\n",
    "            iy   = int((index)/nx) \n",
    "            ix   = index - (iy)*nx\n",
    "            xloc = xmn + (ix)*xsiz\n",
    "            yloc = ymn + (iy)*ysiz\n",
    "        else:\n",
    "            ddh = 0.0\n",
    "            # TODO: pass the cross validation value\n",
    "\n",
    "# Find the nearest samples within each octant: First initialize the counter arrays:\n",
    "        na = -1   # accounting for 0 as first index\n",
    "        dist.fill(1.0e+20)\n",
    "        nums.fill(-1)\n",
    "        current_node = (yloc,xloc)\n",
    "        print('current location ' + str(current_node))\n",
    "        dist, close = tree.query(current_node,ndmax) # use kd tree for fast nearest data search\n",
    "        # remove any data outside search radius\n",
    "        close = close[dist<radius]\n",
    "        dist = dist[dist<radius]     \n",
    "        \n",
    "        print('close' + str(close))\n",
    "        print('dist' + str(dist))\n",
    "        \n",
    "        nclose = len(dist) \n",
    "\n",
    "# Is there enough samples?\n",
    "\n",
    "        if nclose < ndmin:   # accounting for min index of 0\n",
    "            for i in range(0,ncut):\n",
    "                ccdfo[i] = UNEST\n",
    "            print('UNEST at ' + str(ix) + ',' + str(iy))\n",
    "        else:         \n",
    "\n",
    "# Loop over all the thresholds/categories:\n",
    "            for ic in range(0,ncut):\n",
    "                nclose = len(dist)\n",
    "                print()\n",
    "                print('Threshold ' + str(ic))\n",
    "                krig = True\n",
    "                if mik == 1 and ic >= 1: krig = False\n",
    "\n",
    "# Identify the close data (there may be a different number of data at\n",
    "# each threshold because of constraint intervals); however, if\n",
    "# there are no constraint intervals then this step can be avoided.\n",
    "                nca = -1\n",
    "                for ia in range(0,nclose):\n",
    "                    j  = int(close[ia]+0.5)\n",
    "                    ii = actloc[j]\n",
    "                    accept = True\n",
    "                    if koption != 0 and (abs(x[j]-xloc) + abs(y[j]-yloc)).lt.EPSLON: \n",
    "                        print('offset reject')\n",
    "                        accept = False\n",
    "#                     print('Vr ' + str(vr[ii,ic]))\n",
    "                    if vr[ii,ic] < -9990: # remove null values, soft contraints\n",
    "                        print('Data Removed ' + str(ii))\n",
    "                        accept = False\n",
    "                    if accept:\n",
    "#                         print('Accepted ' + str(vr[ii,ic]))\n",
    "                        nca = nca + 1\n",
    "                        vra[nca] = vr[ii,ic]\n",
    "                        xa[nca]  = x[j]\n",
    "                        ya[nca]  = y[j]\n",
    "\n",
    "                        \n",
    "                print('nca ' + str (nca))\n",
    "                print('vra ' +  str (vra))\n",
    "                print('xa ' + str (xa))\n",
    "                print('ya ' + str (ya))\n",
    "# If there are no samples at this threshold then use the global cdf:\n",
    "                if nca == -1:\n",
    "                    if use_trend:\n",
    "                        ccdf[ic] = trend[ny-iy-1,ix,ic]\n",
    "                    else:\n",
    "                        ccdf[ic] = gcdf[ic]\n",
    "                else:\n",
    "            \n",
    "# Now, only load the variogram, build the matrix,... if kriging:\n",
    "                    nclose = nca + 1\n",
    "                    neq = nclose + ktype\n",
    "                    na = nclose\n",
    "                    print('nclose ' + str(nclose))\n",
    "\n",
    "# Set up kriging matrices:\n",
    "                    iin=-1 # accounting for first index of 0\n",
    "                    for j in range(0,na):\n",
    "# Establish Left Hand Side Covariance Matrix:\n",
    "                        for i in range(0,na):  # was j - want full matrix                    \n",
    "                            iin = iin + 1\n",
    "                            a[iin] = cova2(xa[i],ya[i],xa[j],ya[j],nst[ic],c0[ic],PMX,cc[:,ic],aa[:,ic],it[:,ic],ang[:,ic],anis[:,ic],rotmat[ic],maxcov[ic]) \n",
    "                        if ktype == 1:\n",
    "                            iin = iin + 1\n",
    "                            a[iin] = maxcov[ic]            \n",
    "                        r[j] = cova2(xloc,yloc,xa[j],ya[j],nst[ic],c0[ic],PMX,cc[:,ic],aa[:,ic],it[:,ic],ang[:,ic],anis[:,ic],rotmat[ic],maxcov[ic]) \n",
    "    \n",
    "# Set the unbiasedness constraint:\n",
    "                    if ktype == 1:\n",
    "                        for i in range(0,na):\n",
    "                            iin = iin + 1\n",
    "                            a[iin] = maxcov[ic]\n",
    "                        iin      = iin + 1\n",
    "                        a[iin]   = 0.0\n",
    "                        r[neq-1]  = maxcov[ic]\n",
    "                        rr[neq-1] = r[neq]\n",
    "# Solve the system:\n",
    "                    if neq == 1:\n",
    "                        ising = 0.0\n",
    "                        s[0]  = r[0] / a[0]\n",
    "                    else:\n",
    "                        s = ksol_numpy(neq,a,r)\n",
    "\n",
    "# Finished kriging (if it was necessary):\n",
    "\n",
    "# Compute Kriged estimate of cumulative probability:\n",
    "                    sumwts   = 0.0\n",
    "                    ccdf[ic] = 0.0\n",
    "                    #print('Use trend ' + str(use_trend))\n",
    "                    for i in range(0,nclose):\n",
    "                        ccdf[ic] = ccdf[ic] + vra[i]*s[i]\n",
    "                        sumwts   = sumwts   + s[i]\n",
    "                        \n",
    "                    print('Kriging:')    \n",
    "                    print('sumwts' + str(sumwts))\n",
    "                    print('nclose ' + str(nclose))\n",
    "                    print(vra[:nclose])\n",
    "                    print('Weights ' + str(s[:nclose]))\n",
    "                    print('Kriging Estimate ' + str(ccdf[ic]))\n",
    "                    \n",
    "                    if ktype == 0: \n",
    "                        if use_trend == True:\n",
    "                            ccdf[ic] = ccdf[ic] + (1.0-sumwts)*trend[ny-iy-1,ix,ic]\n",
    "                        else:\n",
    "                            ccdf[ic] = ccdf[ic] + (1.0-sumwts)*gcdf[ic]\n",
    "\n",
    "                    print('Kriging Estimate After Global ' + str(ccdf[ic]))                            \n",
    "                            \n",
    "# Keep looping until all the thresholds are estimated:\n",
    " \n",
    "# Correct and write the distribution to the output file:\n",
    "            print('Before order relations ' + str(ccdf))\n",
    "            nk = nk + 1\n",
    "            ccdfo = ordrel(ivtype,ncut,ccdf)\n",
    "            print('After order relations ' + str(ccdfo))\n",
    "        \n",
    "# Write the IK CCDF for this grid node:\n",
    "            if koption == 0:\n",
    "                print('iy ' + str(ny-iy-1) + ' ix ' + str(ix))\n",
    "                print('print shape output ' + str(ikout.shape))\n",
    "                print('ny' + str(ny) + ' nx ' + str(nx))\n",
    "                ikout[ny-iy-1,ix,:] = ccdfo\n",
    "            else:\n",
    "                 print('TBD')\n",
    "                    \n",
    "#             if ix == 170:\n",
    "#                 asl;ghal;g\n",
    "    return ikout  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example lets just assume the indicator variograms for the 2 facies, 0 is shale and 1 is sand.  We declare a list and then add the variograms in the order of the categories in the thresh and global cdf arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = 0.0; xmax = 1000.0               # range of x values\n",
    "ymin = 0.0; ymax = 1000.0               # range of y values\n",
    "radius = 10000\n",
    "ivtype = 1\n",
    "ktype = 0\n",
    "xsiz = 10; ysiz = 10\n",
    "nx = 200; ny = 1\n",
    "xmn = 0; ymn = 1000\n",
    "xsiz = 10; ysiz = 10\n",
    "tmin = 10; tmax = 30; ndmin = 0; ndmax = 9\n",
    "\n",
    "ncut = 11                                   # number of facies\n",
    "thresh = [10,12,14,16,18,20,22,24,26,28,30]                             # the facies categories\n",
    "gcdf =   [0.03,0.06,0.15,0.30,0.50,0.65,0.83,0.87,0.91,0.93,0.97]                           # the global proportions of the categories\n",
    "varios = [] # the variogram list\n",
    "for i in range(0,12):\n",
    "    varios.append(GSLIB.make_variogram(nug=0.0,nst=1,it1=1,cc1=1.0,azi1=0,hmaj1=1000,hmin1=1000)) # shale indicator variogram\n",
    "#varios.append(GSLIB.make_variogram(nug=0.0,nst=1,it1=1,cc1=1.0,azi1=0,hmaj1=50,hmin1=40)) # sand indicator variogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "readind = True\n",
    "colstart = 5\n",
    "ikmap = ik2d(df,'x','y','z',readind,colstart,ivtype,0,ncut,thresh,gcdf,np.zeros([5,5]),tmin,tmax,nx,xmn,xsiz,ny,ymn,ysiz,\n",
    "         ndmin,ndmax,radius,ktype,vario=varios)\n",
    "#def ik2d(df,xcol,ycol,vcol,readind,colstart,ivtype,koption,ncut,thresh,gcdf,trend,tmin,tmax,nx,xmn,xsiz,ny,ymn,ysiz,\n",
    "#         ndmin,ndmax,radius,ktype,vario): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cprob = np.zeros((nx,10))\n",
    "xgrid = np.linspace(xmn,nx*xsiz-xmn,nx)\n",
    "\n",
    "plt.subplot(211)\n",
    "for ix, x in enumerate(np.linspace(xmn,nx*xsiz-xmn,nx)):\n",
    "    for icumprob, cumprob in enumerate(np.linspace(0.05,0.95,10)):\n",
    "        cprob[ix,icumprob] = np.interp(cumprob,ikmap[0,ix,:],thresh,left=None, right=None, period=None)\n",
    "\n",
    "xnew = np.linspace(0.0,2000,300); plevels = np.linspace(0.05,0.95,10)\n",
    "spline_surf = np.zeros([len(xnew),len(plevels)])\n",
    "        \n",
    "\n",
    "for icumprob, cumprob in enumerate(plevels):\n",
    "    #plt.plot(xgrid,cprob[:,icumprob],color='grey',alpha=0.3,zorder=10) \n",
    "    spline = make_interp_spline(xgrid,cprob[:,icumprob], k=2)\n",
    "    alpha = 1-abs(cumprob-0.5); lw = (1-abs(cumprob-0.5)*1.5)*1.5; \n",
    "    spline_surf[:,icumprob] = spline(xnew) \n",
    "    plt.plot(xnew[1:-1],spline_surf[:,icumprob][1:-1],color='black',alpha=alpha,lw =lw,zorder=2000)\n",
    "    \n",
    "plt.fill_between(xnew,spline_surf[:,2],spline_surf[:,3],color='yellow',alpha=0.5,zorder=1,label='P25-P75')\n",
    "plt.fill_between(xnew,spline_surf[:,3],spline_surf[:,4],color='orange',alpha=0.5,zorder=1,label='P35-P65')\n",
    "plt.fill_between(xnew,spline_surf[:,4],spline_surf[:,5],color='red',alpha=0.5,zorder=1,label='P45-P55')\n",
    "plt.fill_between(xnew,spline_surf[:,5],spline_surf[:,6],color='orange',alpha=0.5,zorder=1)\n",
    "plt.fill_between(xnew,spline_surf[:,6],spline_surf[:,7],color='yellow',alpha=0.5,zorder=1)\n",
    "plt.legend(loc = 'upper left')\n",
    "                 \n",
    "for idata in range(0,len(df)):\n",
    "    plt.vlines(df['x'][idata],10,30.0,color='black',ls='--',zorder=10000)\n",
    "    \n",
    "# plt.legend(loc='upper right')\n",
    "    \n",
    "plt.ylim([10,30]); plt.xlim([0,2000]); plt.xlabel('X (m)'); plt.ylabel('Z (m)'); plt.title('Soft Well Data Bounding Surface Constraints - PDFs')\n",
    "\n",
    "for idata, zmin in enumerate(df['zmin']):\n",
    "    zmax = df['zmax'][idata]\n",
    "    if zmin == zmax:\n",
    "        plt.scatter(df['x'][idata],zmin,c='white',edgecolor='black',s=30,zorder=100000)\n",
    "    if zmin < -9990:\n",
    "        plt.plot([df['x'][idata],df['x'][idata]],[30,zmax],color='black',lw=5,zorder=100000)\n",
    "        plt.plot([df['x'][idata],df['x'][idata]],[30,zmax],color='white',lw=3,zorder=100001)\n",
    "    if zmax < -9990:\n",
    "        plt.plot([df['x'][idata],df['x'][idata]],[10,zmin],color='black',lw=5,zorder=100000)\n",
    "        plt.plot([df['x'][idata],df['x'][idata]],[10,zmin],color='white',lw=3,zorder=100001)\n",
    "        \n",
    "add_grid()\n",
    "        \n",
    "plt.subplot(212)\n",
    "for icut in range(0,ncut):\n",
    "#    print(thresh[icut])\n",
    "    for ix in range(1,nx-1):\n",
    "#         print(xgrid[ix],thresh[icut])\n",
    "        im = plt.scatter(xgrid[ix],thresh[icut],s=30,color = plt.cm.inferno(ikmap[0,ix,icut]),marker='s',cmap = plt.cm.inferno,zorder=100)\n",
    "\n",
    "for ix, x in enumerate(np.linspace(xmn,nx*xsiz-xmn,nx)):\n",
    "    for icumprob, cumprob in enumerate(np.linspace(0.05,0.95,10)):\n",
    "        cprob[ix,icumprob] = np.interp(cumprob,ikmap[0,ix,:],thresh,left=None, right=None, period=None)\n",
    "    \n",
    "for idata in range(0,len(df)):\n",
    "    plt.vlines(df['x'][idata],10,30.0,color='black',ls='--',zorder=10000)\n",
    "    \n",
    "delta = 0.3\n",
    "for ithresh, thr in enumerate(thresh):\n",
    "    plt.plot([0,2000],[thr-delta,thr-delta],color='black'); plt.plot([0,2000],[thr+delta,thr+delta],color='black') \n",
    "    \n",
    "# plt.legend(loc='upper right')\n",
    "    \n",
    "plt.ylim([10,30]); plt.xlim([0,2000]); plt.xlabel('X (m)'); plt.ylabel('Z (m)'); plt.title('Soft Well Data Bounding Surface Constraints - Indicator Kriged Probabilities')\n",
    "\n",
    "for idata, zmin in enumerate(df['zmin']):\n",
    "    zmax = df['zmax'][idata]\n",
    "    if zmin == zmax:\n",
    "        plt.scatter(df['x'][idata],zmin,c='white',edgecolor='black',s=30,zorder=100000)\n",
    "    if zmin < -9990:\n",
    "        plt.plot([df['x'][idata],df['x'][idata]],[30,zmax],color='black',lw=5,zorder=100000)\n",
    "        plt.plot([df['x'][idata],df['x'][idata]],[30,zmax],color='white',lw=3,zorder=100001)\n",
    "    if zmax < -9990:\n",
    "        plt.plot([df['x'][idata],df['x'][idata]],[10,zmin],color='black',lw=5,zorder=100000)\n",
    "        plt.plot([df['x'][idata],df['x'][idata]],[10,zmin],color='white',lw=3,zorder=100001)\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.inferno)\n",
    "sm.set_clim(vmin=0, vmax=1.0)\n",
    "cbar = plt.colorbar(sm, orientation=\"horizontal\", ticks=np.linspace(0.0, 1.0, 6),shrink=0.2)\n",
    "cbar.set_label('Cumulative Probability', rotation=0, labelpad=0.1)\n",
    "add_grid()\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.1, hspace=0.3); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Local Uncertainty Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = [25, 100, 150]\n",
    "\n",
    "for iix in range(0,3):\n",
    "    \n",
    "    plt.subplot(3,1,iix+1)\n",
    "    \n",
    "    plt.plot(thresh,ikmap[0,ix[iix],:],color='black',lw=4,zorder=1)\n",
    "    plt.plot(thresh,ikmap[0,ix[iix],:],color='red',lw=2,zorder=2)\n",
    "    plt.scatter(thresh,ikmap[0,ix[iix],:],color='white',edgecolor='black',lw=2,zorder=100)\n",
    "    \n",
    "    plt.xlim([10,30]); plt.ylim([0,1]); plt.xlabel('Z (m)'); \n",
    "    plt.ylabel('Cumulative Probability'); \n",
    "    plt.title('CDF Estimated from Soft Data Constraint at X = ' + str(xmn + ix[iix]*xsiz))\n",
    "\n",
    "    for i in range(0,ncut):\n",
    "        plt.vlines(thresh[i],0,1,color='black',ls='--',lw=1)\n",
    "\n",
    "    add_grid()\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=3.0, wspace=0.2, hspace=0.3); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-field Simulation\n",
    "\n",
    "Extremely flexible method to calculate stochastic simulation from local PDFs, that separates the correlation and local conditioning. \n",
    "\n",
    "* Exceptional ability to honor additional information source\n",
    "\n",
    "* Very good diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfields\n",
    "nxdis = 1; nydis = 1; nreal = 100\n",
    "ndmin = 0; ndmax = 20; radius = 10000; skmean = 0\n",
    "vario = GSLIB.make_variogram(nug=0.001,nst=1,it1=3,cc1=0.999,azi1=90.0,hmaj1=1500,hmin1=1)\n",
    "tmin = -999; tmax = 999\n",
    "\n",
    "dfnull = pd.DataFrame({'X':np.random.rand(1000)-100000, 'Y':np.random.rand(1000)-100000,'pvalue':np.random.rand(1000)})\n",
    "\n",
    "pfield = geostats.sgsim(dfnull,'X','Y','pvalue',wcol=-1,scol=-1,tmin=tmin,tmax=tmax,itrans=1,ismooth=0,dftrans=0,tcol=0,\n",
    "            twtcol=0,zmin=0.0,zmax=1.0,ltail=1,ltpar=0.0,utail=1,utpar=1.0,nsim=1,\n",
    "            nx=nx,xmn=xmn,xsiz=xsiz,ny=nreal,ymn=ymn,ysiz=ysiz,seed=73073,\n",
    "            ndmin=ndmin,ndmax=ndmax,nodmax=50,mults=1,nmult=3,noct=-1,radius=radius,radius1=100000,sang1=0,\n",
    "            mxctx=201,mxcty=81,ktype=0,colocorr=0.0,sec_map=0,vario=vario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out 1D correlated P-fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_pfield = np.zeros([nreal,nx])\n",
    "kernel = np.ones([3])/3.0\n",
    "\n",
    "for ireal, real in enumerate(np.arange(0,nreal,1)): # smoothing\n",
    "    smooth_pfield[ireal,:] = np.convolve(pfield[ireal,:],kernel,mode = 'same') \n",
    "    \n",
    "plt.subplot(111)\n",
    "for ireal, real in enumerate(np.arange(0,10,1)): # smoothing\n",
    "    for ix in range(1,nx-1):\n",
    "        im = plt.scatter(xgrid[ix],ireal+1,s=30,color = plt.cm.inferno(smooth_pfield[ireal,ix]),marker='s',cmap = plt.cm.inferno,zorder=100)\n",
    "\n",
    "delta = 0.13\n",
    "for ireal, real in enumerate(np.arange(0,10,1)): # smoothing\n",
    "    plt.plot([0,2000],[ireal-delta,ireal-delta],color='black'); plt.plot([0,2000],[ireal+delta,ireal+delta],color='black') \n",
    "         \n",
    "plt.xlabel('X (m)'); plt.ylabel('Relization (index)')\n",
    "plt.ylim([1,10]); plt.xlim([0,2000]); plt.xlabel('X (m)'); plt.title('P-feilds - Correlated Probability Fields')\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.inferno)\n",
    "sm.set_clim(vmin=0, vmax=1.0)\n",
    "cbar = plt.colorbar(sm, orientation=\"horizontal\", ticks=np.linspace(0.0, 1.0, 6),shrink=0.2)\n",
    "cbar.set_label('P-value Realizations', rotation=0, labelpad=0.1)\n",
    "add_grid()\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply our P-fields to our local uncertainty distributions from our hard and soft well data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nreal = 10\n",
    "\n",
    "kernel = np.ones([3])/3.0\n",
    "\n",
    "sim = np.zeros([nx,nreal]); smooth_sim = np.zeros([nx,nreal])\n",
    "for ireal, real in enumerate(np.arange(0,nreal,1)):\n",
    "    for ix, x in enumerate(np.linspace(xmn,nx*xsiz-xmn,nx)):\n",
    "        sim[ix,ireal] = np.interp(pfield[ireal,ix],ikmap[0,ix,:],thresh,left=None, right=None, period=None)  \n",
    "\n",
    "for ireal, real in enumerate(np.arange(0,nreal,1)): # smoothing\n",
    "    smooth_sim[:,ireal] = np.convolve(sim[:,ireal],kernel,mode = 'same') \n",
    "        \n",
    "plt.subplot(2,1,1)\n",
    "for ireal, real in enumerate(np.arange(0,nreal,1)):\n",
    "    plt.plot(np.linspace(xmn,nx*xsiz-xmn,nx)[1:-1],pfield[ireal,1:-1],alpha=1.0,\n",
    "            color = plt.cm.inferno(ireal/(nreal-1)),lw=3,zorder=10)\n",
    "    plt.plot(np.linspace(xmn,nx*xsiz-xmn,nx)[1:-1],pfield[ireal,1:-1],alpha=1.0,\n",
    "            color = 'black',lw=4,zorder=1)    \n",
    "add_grid()\n",
    "    \n",
    "plt.ylim([0,1]); plt.xlim([0,2000]); plt.xlabel('X (m)'); plt.ylabel('Z (m)'); plt.title('Stochastic P-fields')\n",
    "\n",
    "plt.subplot(2,1,2)        \n",
    "for ireal, real in enumerate(np.arange(0,nreal,1)):\n",
    "    plt.plot(np.linspace(xmn,nx*xsiz-xmn,nx)[1:-1],smooth_sim[1:-1,ireal],alpha=1.0,\n",
    "            color = plt.cm.inferno(ireal/(nreal-1)),lw=3,zorder=10)\n",
    "    plt.plot(np.linspace(xmn,nx*xsiz-xmn,nx)[1:-1],smooth_sim[1:-1,ireal],alpha=1.0,\n",
    "            color = 'black',lw=4,zorder=1) \n",
    "\n",
    "    \n",
    "for idata in range(0,len(df)):\n",
    "    plt.vlines(df['x'][idata],10,30.0,color='black',ls='--',zorder=10000)\n",
    "\n",
    "for idata, zmin in enumerate(df['zmin']):\n",
    "    zmax = df['zmax'][idata]\n",
    "    if zmin == zmax:\n",
    "        plt.scatter(df['x'][idata],zmin,c='white',edgecolor='black',s=30,zorder=100000)\n",
    "    if zmin < -9990:\n",
    "        plt.plot([df['x'][idata],df['x'][idata]],[30,zmax],color='black',lw=5,zorder=100000)\n",
    "        plt.plot([df['x'][idata],df['x'][idata]],[30,zmax],color='white',lw=3,zorder=100001)\n",
    "    if zmax < -9990:\n",
    "        plt.plot([df['x'][idata],df['x'][idata]],[10,zmin],color='black',lw=5,zorder=100000)\n",
    "        plt.plot([df['x'][idata],df['x'][idata]],[10,zmin],color='white',lw=3,zorder=100001)\n",
    "add_grid()\n",
    "\n",
    "plt.ylim([10,30]); plt.xlim([0,2000]); plt.xlabel('X (m)'); plt.ylabel('Z (m)'); plt.title('Soft Well Data Bounding Surface Simulations')\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.2, hspace=0.5); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Michael*\n",
    "\n",
    "**Michael Pyrcz**, Ph.D., P.Eng. Professor The Hildebrand Department of Petroleum and Geosystems Engineering, Bureau of Economic Geology, The Jackson School of Geosciences, The University of Texas at Austin\n",
    "On Twitter I'm the **GeostatsGuy** and on YouTube my lectures are on the channel, **GeostatsGuy Lectures**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
